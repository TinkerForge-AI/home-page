---
title: "AI Alignment Experiment: Measuring Model Robustness"
description: "A hands-on experiment to evaluate the robustness of large language models to adversarial prompts and distributional shifts."
date: "2025-07-10"
author: "TinkerForge AI"
tags: ["alignment", "robustness", "experiment"]
category: "AI Safety"
image: "/images/ai-alignment-experiment.png"
---

In this experiment, we test several open-source language models for robustness to adversarial prompts and out-of-distribution questions. Our methodology, results, and code are all open for review and reuse.

Key findings:
- Some models are more robust to prompt injection than others.
- Fine-tuning on adversarial data improves performance.
- See our [GitHub repo](https://github.com/tinkerforge-ai/ai-alignment-experiment) for code and data.